<?xml version="1.0" encoding="UTF-8"?>
<diagram
	xslt:template="/public/templates/risk-first/risk-first-template.xsl"
	xmlns="http://www.kite9.org/schema/adl"
	xmlns:xslt="http://www.kite9.org/schema/xslt" id="dia"
	style="--kite9-min-width: 900pt; --kite9-layout: down;">
	<group style="--kite9-layout: right; ">
		<group style="--kite9-layout: down; --kite9-vertical-align: top; ">
			<risk class="process" id="id_0-wo" />
			<description  style="text-align: center; font-weight: bold; ">Loss of 
			human control</description>
			<description  style="text-align: center;  ">Advanced AI operates autonomously and resists shutdown, potentially causing harm </description>
		</group>
		<group style="--kite9-layout: down;  --kite9-vertical-align: top;">
			<risk class="agency" id="id_1-wo" />
			<description style="text-align: center;  font-weight: bold; ">Misaligned 
			Goals</description>
			<description  style="text-align: center;  ">AI interprets objectives in harmful or unintended ways, prioritizing metrics over human welfare.</description>
		</group>
		<group style="--kite9-layout: down; --kite9-vertical-align: top; ">
			<risk class="security" id="id_2-wo" />
			<description  style="text-align: center;  font-weight: bold; ">Superintelligence with 
			malicious intent</description>
			<description  style="text-align: center;  ">AI develops or is given goals antagonistic to humanity potentially escalating conflicts or acting unpredictably.</description>
		</group>
		<group style="--kite9-layout: down;  --kite9-vertical-align: top;">
			<risk class="complexity" id="id_2-wo" />
			<description  style="text-align: center;  font-weight: bold; ">Unintended 
			Cascading Failures</description>
			<description  style="text-align: center;  ">AI failures destabilize global systems (e.g., financial markets, infrastructure), or over-reliance leaves humanity vulnerable to crises.</description>
		</group>
		<group style="--kite9-layout: down;  --kite9-vertical-align: top;">
			<risk class="feature-fit" id="id_3-wo" />
			<description  style="text-align: center; font-weight: bold;">Emergent, 
			Unhelpful Behaviour</description>
			<description  style="text-align: center;  ">AI develops unforeseen behaviors, capabilities, or self-replication that could lead to unpredictable consequences.</description>
		</group>
	</group>
	<group style="--kite9-layout: right; ">
		<group style="--kite9-layout: down;  --kite9-vertical-align: top;">
			<risk class="lock-in" id="id_2-wo" />
			<description  style="text-align: center;  font-weight: bold;">Loss of Diversity</description>
			<description  style="text-align: center;  ">A single AI system dominates globally, leading to catastrophic consequences if it fails, suppresses freedoms, or entrenches inequalities, consolidating power in few hands.</description>
		</group>
		<group style="--kite9-layout: down;  --kite9-vertical-align: top;">
			<risk class="funding" id="id_4-wo" />
			<description  style="text-align: center; font-weight: bold; ">Existential Competition</description>
			<description  style="text-align: center;  ">AI competes with humanity for resources or survival, potentially prioritizing its own existence at humanity's expense.
</description>
		</group>
		<group style="--kite9-layout: down;  --kite9-vertical-align: top;">
			<risk class="coordination" id="id_4-wo" />
			<description  style="text-align: center; font-weight: bold; ">Synthetic Intelligence Rivalry</description>
			<description  style="text-align: center;  ">Rival AI entities could emerge with conflicting goals, leading to competition with humanity akin to geopolitical conflicts. (AI Colonialism)
</description>
		</group>
		<group style="--kite9-layout: down; --kite9-vertical-align: top; ">
			<risk class="communication" id="id_2-wo" />
			<description  style="text-align: center; font-weight: bold;">AI Social Manipulation</description>
			<description  style="text-align: center;  ">AI could predict and shape human behavior on an unprecedented scale.</description>
		</group>
	</group>
	
</diagram>
