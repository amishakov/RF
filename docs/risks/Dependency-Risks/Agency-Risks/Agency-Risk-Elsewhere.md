
## Agency Elsewhere

In the examples above, we've looked at hierarchy of goals for _most people_.  It doesn't always play out like this and the structure is quite fluid.  Some examples:

 - In 2018, a 15-year-old [Greta Thunberg](https://en.wikipedia.org/wiki/Greta_Thunberg) gave up her education goals to campaign outside parliament in Sweden.  She is now widely recognised as a key figure in climate activism. 
 - Steve Jobs, despite designing amazing hardware at Apple Computers, was a self-confessed [terrible father](https://en.wikipedia.org/wiki/Steve_Jobs#Family) and [failed to look after himself when diagnosed with cancer](https://en.wikipedia.org/wiki/Steve_Jobs#Health_problems).
 - Less specifically, soldiers often form very close bonds due to their reliance on each other for survival, akin to family members (_brothers in arms_).  

### Animals

Given the fluidity of the goal hierarchy for people, we shouldn't be surprised that other animals don't have the same priorities.  For example, [Colobopsis Saundersi](https://en.wikipedia.org/wiki/Colobopsis_saundersi) is a species of ant that can explode suicidally and aggressively as an ultimate act of defence.  Given that individual ants are not capable of reproduction, it seems to make sense that they would sacrifice themselves for the good of the colony:  to _not_ do so would reduce the colony's chance of surviving and reproducing. 

### Software Processes

![Software Goals](/img/generated/risks/agency/software.svg) 

Compared to humans, most software has a simple goal hierarchy, as shown in the diagram above.  Nevertheless, there is significant [Agency Risk](/tags/Agency-Risk) in running software _at all_.  Since computer systems follow rules we set for them, we shouldn't be surprised when those rules have exceptions that lead to disaster.  For example:

 - A process continually writing log files until the disks fill up, crashing the system.
 - Bugs causing data to get corrupted, causing financial loss.
 - Malware exploiting weaknesses in a system, exposing sensitive data.
 
### Paperclips

Building software systems that try to optimise for a hierarchy of goals (like humans do) is still a research project.  But it is something AI researchers such as [Nick Bostrom](https://en.wikipedia.org/wiki/Nick_Bostrom) worry about.  Consider his AI thought experiment:

> "If you give an artificial intelligence an explicit goal – like maximizing the number of paper clips in the world – and that artificial intelligence has gotten smart enough to the point where it is capable of inventing its own super-technologies and building its own manufacturing plants, then, well, be careful what you wish for." -- [Nick Bostrom, _Wikipedia_](https://en.wikipedia.org/wiki/Universal_Paperclips#Themes)

![Universal Paperclips](/img/generated/risks/agency/paperclips.svg)

Bostrom worries that humanity would be steamrollered accidentally whilst trying to maximise the paperclip goal.  The AI need not be malevolent - it's enough that it just requres the resources that keep us alive!

This problem may be a long way off.  In any case it's not really in our interests to build AI systems that prioritise their own survival.  As humans, we have inherited the survival goal through evolution: an AI wouldn't necessarily have this goal unless we subjected AI development to some kind of evolutionary survival-of-the-fittest process too.
