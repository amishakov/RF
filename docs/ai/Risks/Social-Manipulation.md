---
title: Social Manipulation
description: AI could predict and shape human behaviour on an unprecedented scale.

featured: 
  class: c
  element: '<risk class="communication">Social Manipulation</risk>'
tags:
 - AI-Risk
 - Social-Manipulation
sidebar_position: 2
tweet: yes
---

AI systems designed to influence behaviour at scale could (and do) undermine democracy, free will, and individual autonomy.

## Sources

- **The spread of true and false news online** [Vosoughi, Roy, & Aral, 2018](https://doi.org/10.1126/science.aap9559): Demonstrates that false news travels faster and reaches more people than true news on social platforms, indicating how AI-driven disinformation campaigns could exploit these vulnerabilities.

- **The science of fake news** [Lazer et al., 2018](https://www.researchgate.net/publication/323650280_The_science_of_fake_news/link/5b30d8760f7e9b0df5c767b7/download?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InB1YmxpY2F0aW9uIiwicGFnZSI6InB1YmxpY2F0aW9uIn19): Examines the ecosystem of fake news creation and dissemination, emphasising the critical need for policy, research, and technological measures to counter AI-enabled misinformation.  Also, recognises how at odds any measures of control are with the purest notions of "free speech".

- **Deepfakes: A Grounded Threat Assessment** [Chesney & Citron, 2019](https://dx.doi.org/10.2139/ssrn.3213954): Explores the implications of deepfake technology, highlighting the urgent necessity for innovative detection tools and policy interventions (see below).

- **Nazi Propaganda** [United States Holocaust Memorial Museum](https://encyclopedia.ushmm.org/content/en/article/nazi-propaganda): Examines how the Nazi regime harnessed mass media—including radio broadcasts, film, and print—to shape public opinion, consolidate power, and foment anti-Semitic attitudes during World War.  (Fake content isn't a new problem.)

---

## How This Is Already Happening

### AI-Powered Targeted Advertising & Manipulation

- Algorithms mine user data to deliver highly customised ads.
- Personalized messaging exploits individual biases, vulnerabilities, or preferences.

**Real-Life Examples:**

- The [Cambridge Analytica scandal (2016)](https://en.wikipedia.org/wiki/Facebook–Cambridge_Analytica_data_scandal): Data from millions of Facebook users was used to create highly customized political ads, influencing voter perceptions in the U.S. and beyond.
- Political microtargeting in multiple elections: Campaigns worldwide have leveraged platforms like Meta (Facebook) or Google to deliver personalized messages designed to sway opinion on sensitive issues.

### AI-Generated Disinformation & Deepfakes

- Sophisticated tools create realistic but false content that distorts public perception.
- Deepfake videos or audio can undermine trust in legitimate information sources.

**Real-Life Examples:**

- [Fake Zelensky video (2022)](https://www.npr.org/2022/03/16/1087062648/deepfake-video-zelenskyy-experts-war-manipulation-ukraine-russia): A deepfake video urged Ukrainian forces to surrender, illustrating how synthetic media can be weaponized during international conflicts.
- Fake celebrity endorsements: AI-generated videos and images appear online, falsely promoting products or political messages by well-known public figures. e.g. [2022 Elon Musk Crypto Scam](https://www.vice.com/en/article/scammers-use-elon-musk-deepfake-to-steal-crypto/): A deepfake video circulated on social media, featuring a convincing impersonation of Elon Musk endorsing a fraudulent cryptocurrency platform. 
- [2019 Deepfake CEO Phone Scam](https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-deepfake-was-used-to-scam-a-ceo-out-of-243000/): In a widely reported case, criminals used AI-generated voice to impersonate a chief executive, successfully convincing a subordinate to transfer \$243,000 to a fraudulent account. 

### Predictive AI Systems Controlling Social Behavior

- Platforms use behavioral models to shape user experiences, potentially pushing them to adopt certain viewpoints or behaviors.
- Data-driven predictions about habits and preferences can be used to modify or influence choices.

**Real-Life Examples:**

- TikTok’s recommendation algorithm: Known for its powerful engagement-driven feed, it can rapidly shape users’ content consumption, potentially reinforcing certain narratives or trends. See: [TikTok Algorithm Eating Disorder article on The Verge](https://www.theverge.com/2021/12/18/22843606/tiktok-wsj-algorithm-change-eating-disorder).

- China’s social credit initiatives: Although not purely about content manipulation, these systems use data and behavioral metrics to encourage or discourage particular actions, effectively guiding social behavior.

---

## Mitigations

### AI Transparency Regulations

- Mandate clear labeling of AI-generated content.
- Require accountability and auditing mechanisms for social media platforms.
- **Examples:** 
  - [Generative AI and watermarking - European Parliament](https://www.europarl.europa.eu/RegData/etudes/BRIE/2023/757583/EPRS_BRI\(2023\)757583_EN.pdf)
- **Efficacy:** Medium – Transparency can deter some manipulative actors, but determined bad actors may still evade or exploit labelling.
- **Ease of Implementation:** Moderate – Requires infrastructure for labelling, auditing, and enforcement, but could be mandated by legislation.

### Ethical AI Development Standards

- Industry-wide codes of conduct to discourage manipulative AI.
- Incentivize designers to embed fairness and user consent into algorithmic systems.
- **Examples**
  -  [Understanding artificial intelligence ethics and safety - Turing Institute](https://www.turing.ac.uk/sites/default/files/2019-06/understanding_artificial_intelligence_ethics_and_safety.pdf)
  - [AI Playbook for the UK Government](https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principles)
  - [DOD Adopts Ethical Principles for Artificial Intelligence](https://www.defense.gov/News/Releases/Release/Article/2091996/dod-adopts-ethical-principles-for-artificial-intelligence/)
- **Efficacy:** Medium – Encourages best practices and self-regulation, but relies on voluntary compliance without legal backing.
- **Ease of Implementation:** Low – Professional bodies and industry coalitions can quickly adopt and publicize guidelines, though ensuring universal adherence remains a challenge. Firms have varying incentives, budgets, and ethical priorities, making universal buy-in elusive.

### Education & Public Awareness Campaigns

- Equip citizens with media literacy skills to spot deepfakes and manipulation attempts.
- Encourage public understanding of how personal data can be exploited by AI-driven systems.
- **Examples:**
  - https://newslit.org
  - https://www.unesco.org/en/media-information-literacy
- **Efficacy:** High – Empowered, media-savvy populations are significantly harder to manipulate.  However, scaling efforts to entire populations is a substantial challenge given diverse educational, cultural, and socioeconomic barriers.
- **Ease of Implementation:** Low – While public outreach is feasible, achieving wide coverage and sustained engagement can be resource-intensive.  Overcoming entrenched biases, misinformation echo chambers, and public apathy is an uphill battle, particularly if there’s no supportive policy or consistent funding.


